{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532d83ac-26e1-4a0a-9aeb-25c9f7b17c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "  paragraph=\"\"\"In today’s digital world, data is generated at an unprecedented rate. From social media platforms to online shopping websites, every action we take leaves a digital footprint. Organizations are now focusing on analyzing this vast amount of data to understand customer behavior, improve user experience, and make informed business decisions. However, processing natural language data is not a simple task. It involves multiple steps such as tokenization, stopword removal, stemming, and lemmatization. With the help of advanced Natural Language Processing techniques, machines are now capable of understanding context, sentiment, and even emotions behind human communication.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b669ad95-023a-4477-8db1-4a783393c73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "   ---------------------------------------- 0.0/15.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/15.5 MB 8.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.5 MB 4.6 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 2.1/15.5 MB 4.5 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/15.5 MB 4.2 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.5/15.5 MB 4.1 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.2/15.5 MB 4.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.0/15.5 MB 4.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 6.8/15.5 MB 4.0 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 7.6/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.4/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.2/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.0/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 11.0/15.5 MB 4.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.8/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 12.8/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 13.6/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 14.4/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.2/15.5 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.5/15.5 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\kavis\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\kavis\\anaconda3\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "streamlit 1.37.1 requires pillow<11,>=7.1.0, but you have pillow 11.3.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.26.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826cc3aa-4fcd-4213-a453-9f2c03cb297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3fa1eea-7f3d-49c8-91a0-a8ce5a468e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Run these once in Jupyter Notebook\n",
    "nltk.download('punkt')        # sentence tokenizer\n",
    "nltk.download('stopwords')    # stopwords\n",
    "nltk.download('wordnet')      # lemmatization\n",
    "nltk.download('omw-1.4')      # wordnet extra data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c887dc9d-9618-4864-a4fd-7001c438ea23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5809fc4-b03e-4321-8ce9-dc2608b62873",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c61f9acf-255e-4694-a7c5-9e0908f38792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In today’s digital world, data is generated at an unprecedented rate.',\n",
       " 'From social media platforms to online shopping websites, every action we take leaves a digital footprint.',\n",
       " 'Organizations are now focusing on analyzing this vast amount of data to understand customer behavior, improve user experience, and make informed business decisions.',\n",
       " 'However, processing natural language data is not a simple task.',\n",
       " 'It involves multiple steps such as tokenization, stopword removal, stemming, and lemmatization.',\n",
       " 'With the help of advanced Natural Language Processing techniques, machines are now capable of understanding context, sentiment, and even emotions behind human communication.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "056e3d25-403e-40d4-9575-f1116a6aa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14d1d3f5-af97-4655-a1cb-87376984acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e4e7b23-97d4-43c2-87e5-5e68a23d1e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec08194d-3299-4a64-884a-76c64a24f5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\kavis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3da5bd66-253f-4a99-8d98-d29f3b7e8c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('today', 'NN'), ('digital', 'JJ'), ('world', 'NN'), ('data', 'NNS'), ('generated', 'VBD'), ('unprecedented', 'JJ'), ('rate', 'NN')]\n",
      "[('social', 'JJ'), ('media', 'NNS'), ('platforms', 'NNS'), ('online', 'VBP'), ('shopping', 'VBG'), ('websites', 'NNS'), ('every', 'DT'), ('action', 'NN'), ('take', 'VB'), ('leaves', 'NNS'), ('digital', 'JJ'), ('footprint', 'NN')]\n",
      "[('Organizations', 'NNS'), ('focusing', 'VBG'), ('analyzing', 'VBG'), ('vast', 'JJ'), ('amount', 'NN'), ('data', 'NNS'), ('understand', 'VBP'), ('customer', 'NN'), ('behavior', 'NN'), ('improve', 'VB'), ('user', 'NN'), ('experience', 'NN'), ('make', 'NN'), ('informed', 'JJ'), ('business', 'NN'), ('decisions', 'NNS')]\n",
      "[('However', 'RB'), ('processing', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('data', 'NNS'), ('simple', 'NN'), ('task', 'NN')]\n",
      "[('involves', 'NNS'), ('multiple', 'JJ'), ('steps', 'NNS'), ('tokenization', 'NN'), ('stopword', 'NN'), ('removal', 'NN'), ('stemming', 'VBG'), ('lemmatization', 'NN')]\n",
      "[('help', 'NN'), ('advanced', 'VBD'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('techniques', 'NNS'), ('machines', 'NNS'), ('capable', 'JJ'), ('understanding', 'VBG'), ('context', 'JJ'), ('sentiment', 'NN'), ('even', 'RB'), ('emotions', 'NNS'), ('behind', 'IN'), ('human', 'JJ'), ('communication', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eng_stop = set(stopwords.words('english'))\n",
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words = [word for word in words if word.lower() not in eng_stop and word.isalpha()]\n",
    "    tag=nltk.pos_tag(words)\n",
    "    print(tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fc492-4c3d-493a-b088-62332c4096e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
