import pandas as pd
import numpy as np 
import  matplotlib .pyplot as plt
import seaborn as sns
import plotly.express as px
import warnings
warnings.filterwarnings("ignore")
%matplotlib inline




df=pd.read_csv("Travel.csv")


df.head()


df.info()


df.isnull().sum()


df.shape





df.columns


df['CustomerID'].value_counts()


df[ 'Gender'].value_counts()


df[ 'Gender']=df[ 'Gender'].replace('Fe Male','Female')


df[ 'MaritalStatus'].value_counts()


df[ 'MaritalStatus']=df[ 'MaritalStatus'].replace('Unmarried','Single')


df['TypeofContact'].value_counts()


features_with_na=[f for f in df.columns if df[f].isnull().sum()>=1]
for i in features_with_na:
    print(i,np.round(df[i].isnull().mean()*100,5),'% missing value')


df[features_with_na].select_dtypes(exclude='object').describe()


df.TypeofContact.fillna(df.TypeofContact.mode()[0],inplace=True)
df.DurationOfPitch.fillna(df.DurationOfPitch.median(),inplace=True)
df.NumberOfFollowups.fillna(df.NumberOfFollowups.mode()[0],inplace=True)
df.PreferredPropertyStar.fillna(df.PreferredPropertyStar.mode()[0],inplace=True)
df.NumberOfTrips.fillna(df.NumberOfTrips.median(),inplace=True)
df.MonthlyIncome.fillna(df.MonthlyIncome.median(),inplace=True)
df.NumberOfChildrenVisiting.fillna(df.NumberOfChildrenVisiting.mode()[0],inplace=True)
df.Age.fillna(df.Age.median(),inplace=True)


df.isnull().sum()


df.head()


df.drop('CustomerID',inplace=True,axis=1)


df['Total_visiting']=df['NumberOfChildrenVisiting']+df['NumberOfPersonVisiting']


df.head()


df.drop(columns=['NumberOfChildrenVisiting','NumberOfPersonVisiting'],inplace=True,axis=1)


num_features=[f for f in df.columns if df[f].dtype!='O']
print ("no of numerical features:",len(num_features))


cat_features=[f for f in df.columns if df[f].dtype=='O']
print ("no of categorical features:",len(cat_features))


discrete_features=[f for f in num_features if len(df[f].unique())<=25]
print ("no of discrete features:",len(discrete_features))


continious_features=[f for f in num_features  if f not in discrete_features]
print ("no of cont.  features:",len(continious_features))


# categorical_features to numeric 
from sklearn.model_selection import train_test_split
x=df.drop(['ProdTaken'],axis=1)
y=df['ProdTaken']



y.value_counts()


x.head()


x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)


x_train


cat_feature = x.select_dtypes(include=["object"]).columns
num_feature = x.select_dtypes(exclude=["object"]).columns

from sklearn .preprocessing import OneHotEncoder,StandardScaler
from sklearn.compose import ColumnTransformer
numeric_transformer=StandardScaler()
oh_transformer=OneHotEncoder(drop='first')
preprocessor=ColumnTransformer(
    [
        ("OneHotEncoder",oh_transformer,cat_feature),
        ("StandardScaler",numeric_transformer,num_feature)
    
    ]
)




x_train_transformed = preprocessor.fit_transform(x_train)



x_test_transformed = preprocessor.transform(x_test)



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report,precision_score,recall_score,f1_score,roc_auc_score

from sklearn.tree import DecisionTreeClassifier



from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in x_train.columns:
    if x_train[col].dtype == 'object':
        le = LabelEncoder()
        x_train[col] = le.fit_transform(x_train[col])
        x_test[col] = le.transform(x_test[col])  # same encoder for test
        label_encoders[col] = le



from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
models = {
    "Logistic Regression":LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier()
}

for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(x_train, y_train)
    
    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)
    
    x_train_accuracy = accuracy_score(y_train, y_train_pred)
    x_train_confusion = confusion_matrix(y_train, y_train_pred)
    x_train_class_report = classification_report(y_train, y_train_pred)
    x_train_precision = precision_score(y_train, y_train_pred, average='weighted')
    x_train_recall = recall_score(y_train, y_train_pred, average='weighted')
    x_train_f1 = f1_score(y_train, y_train_pred, average='weighted')

    x_test_accuracy = accuracy_score(y_test, y_test_pred)
    x_test_confusion = confusion_matrix(y_test, y_test_pred)
    x_test_class_report = classification_report(y_test, y_test_pred)
    x_test_precision = precision_score(y_test, y_test_pred, average='weighted')
    x_test_recall = recall_score(y_test, y_test_pred, average='weighted')
    x_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    
    print(f"\nModel: {list(models.keys())[i]}")
    print("=== Training Set Performance ===")
    print(f"Accuracy: {x_train_accuracy}")
    print(f"Precision: {x_train_precision}")
    print(f"Recall: {x_train_recall}")
    print(f"F1 Score: {x_train_f1}")
    print("Confusion Matrix:\n", x_train_confusion)
    print("Classification Report:\n", x_train_class_report)

    print("=== Testing Set Performance ===")
    print(f"Accuracy: {x_test_accuracy}")
    print(f"Precision: {x_test_precision}")
    print(f"Recall: {x_test_recall}")
    print(f"F1 Score: {x_test_f1}")
    print("Confusion Matrix:\n", x_test_confusion)
    print("Classification Report:\n", x_test_class_report)



# hyperparametrer training 
rf_params={"max_depth":[5,8,15,None,10],
           "max_feature":[5,7,"auto",8],
           "min_samples_split":[2,8,15,20],
           "n_estimators":[100,200,500,1000]}


random_cv_models=[
    ("RF",RandomForestClassifier(),rf_params)
]


from sklearn.model_selection import RandomizedSearchCV


model_params={}
for name, model,params in random_cv_models:
    random=RandomizedSearchCV(estimator=model,param_distributions=params,n_iter=100,cv=3,verbose=2,n_jobs=-1)
    random.fit(x_train,y_train)
    model_params[name]=random.best_params_
for model_name in model_params:
    print(f"-----------------------Best Params for {model_name}-------------------")
    print(model_params[model_name])


models={
    "Random Forest": RandomForestClassifier(n_estimators=1000,min_samples_split=2,max_features=7,max_depth=15)
    
}
for i in range(len(list(models))):
    model = list(models.values())[i]
    model.fit(x_train, y_train)
    
    y_train_pred = model.predict(x_train)
    y_test_pred = model.predict(x_test)
    
    x_train_accuracy = accuracy_score(y_train, y_train_pred)
    x_train_confusion = confusion_matrix(y_train, y_train_pred)
    x_train_class_report = classification_report(y_train, y_train_pred)
    x_train_precision = precision_score(y_train, y_train_pred, average='weighted')
    x_train_recall = recall_score(y_train, y_train_pred, average='weighted')
    x_train_f1 = f1_score(y_train, y_train_pred, average='weighted')

    x_test_accuracy = accuracy_score(y_test, y_test_pred)
    x_test_confusion = confusion_matrix(y_test, y_test_pred)
    x_test_class_report = classification_report(y_test, y_test_pred)
    x_test_precision = precision_score(y_test, y_test_pred, average='weighted')
    x_test_recall = recall_score(y_test, y_test_pred, average='weighted')
    x_test_f1 = f1_score(y_test, y_test_pred, average='weighted')
    
    print(f"\nModel: {list(models.keys())[i]}")
    print("=== Training Set Performance ===")
    print(f"Accuracy: {x_train_accuracy}")
    print(f"Precision: {x_train_precision}")
    print(f"Recall: {x_train_recall}")
    print(f"F1 Score: {x_train_f1}")
    print("Confusion Matrix:\n", x_train_confusion)
    print("Classification Report:\n", x_train_class_report)

    print("=== Testing Set Performance ===")
    print(f"Accuracy: {x_test_accuracy}")
    print(f"Precision: {x_test_precision}")
    print(f"Recall: {x_test_recall}")
    print(f"F1 Score: {x_test_f1}")
    print("Confusion Matrix:\n", x_test_confusion)
    print("Classification Report:\n", x_test_class_report)





















































































































































































